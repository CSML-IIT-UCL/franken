{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Install\n",
    "2. Download (part of?) water dataset.. let's try like this, if too complex then we can go back to storing it.\n",
    "3. Run autotune. How to run hydra from python? See [docs](https://github.com/facebookresearch/hydra/blob/main/examples/advanced/ad_hoc_composition/hydra_compose_example.py)\n",
    "4. compute errors\n",
    "5. Run simple ASE MD for a few steps using the produced model.\n",
    "6. Plot RDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from franken.autotune import autotune\n",
    "from franken.datasets.registry import DATASET_REGISTRY\n",
    "from franken.trainers.rf_cuda_lowmem import RandomFeaturesTrainer\n",
    "from franken.data.base import BaseAtomsDataset\n",
    "from franken.backbones.utils import CacheDir\n",
    "from franken.rf.model import FrankenPotential\n",
    "import franken.metrics\n",
    "from franken.data.base import Target\n",
    "from franken.config import MaceBackboneConfig, MultiscaleGaussianRFConfig\n",
    "from franken.config import AutotuneConfig, DatasetConfig, HPSearchConfig, SolverConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the H2O data obtained from DFT calculations (using RPBE+D3 theory), originally collected by [Montero de Hijes et al.](https://doi.org/10.1063/5.0197105).\n",
    "To showcase the sample efficiency of our model, we finetune a the MACE-L0 foundation model (which in a zero-shot setting has pure accuracy on this data) with only 8 new samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "Here we load 8 random samples from the training set, and the validation set which contains 100 structures.\n",
    "\n",
    "The dataset will be downloaded into franken's cache directory (`CacheDir.get()`).\n",
    "\n",
    "\n",
    "> NOTE: The **cache directory** is used to store downloaded datasets and model backbones.\n",
    "> it defaults to `$HOME/.franken` for the current user and it can be configured by setting the `$FRANKEN_CACHE_DIR`\n",
    "> environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to configure the GNN backbone we wish to use\n",
    "# since the data format depends on which backbone will be loaded.\n",
    "# We will use the MACE-L0 backbone, with features extracted at the 2nd layer\n",
    "gnn_config = MaceBackboneConfig(\n",
    "    path_or_id=\"MACE-L0\",\n",
    "    interaction_block=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuequivariance or cuequivariance_torch is not available. Cuequivariance acceleration will be disabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d167bdc9bb49938400da4166e07c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ASE -> MACE (train):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dset_8 = BaseAtomsDataset.from_path(\n",
    "    data_path=DATASET_REGISTRY.get_path(\"water\", \"train\", base_path=CacheDir.get()),\n",
    "    split=\"train\",\n",
    "    num_random_subsamples=8,\n",
    "    subsample_rng=42,\n",
    "    gnn_config=gnn_config,\n",
    ")\n",
    "train_dl_8 = train_dset_8.get_dataloader(distributed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71938c070c6545b58b7a9da67786c5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ASE -> MACE (val):   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val_dset = BaseAtomsDataset.from_path(\n",
    "    data_path=DATASET_REGISTRY.get_path(\"water\", \"val\", base_path=CacheDir.get()),\n",
    "    split=\"val\",\n",
    "    gnn_config=gnn_config,\n",
    ")\n",
    "val_dl = val_dset.get_dataloader(distributed=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a single model\n",
    "\n",
    "We start in the most explicit setting where all the hyperparameters are set before-hand. Below we describe them one by one in detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first two parameters describe the **backbone model**: MACE-L0, with features extracted at the 2nd layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn_backbone_id = \"MACE-L0\"\n",
    "interaction_block = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we **set the kernel**, and its hyperparameters.\n",
    "Here we use the multiscale Gaussian kernel, which can fit a range of length-scales $\\sigma$ with a single model.\n",
    "This has the big advantage of requiring little tuning: a range of sensible values of $\\sigma$ can be set through the three parameters `\"length_scale_low\", \"length_scale_high\", \"length_scale_num\"`. This is much easier than precisely setting the optimal length0scale, and retains most of the accuracy.\n",
    "\n",
    "The other parameter is the number of random features, which can generally be set based on how much computing time and memory is available: more random features increase accuracy monotonically, although diminishing returns kick in after $\\approx 16000$ random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_config = MultiscaleGaussianRFConfig(\n",
    "    num_random_features=512,\n",
    "    length_scale_low=4.0,\n",
    "    length_scale_high=24.0,\n",
    "    length_scale_num=4,\n",
    "    rng_seed=42,  # for reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **linear system hyperparameters** are:\n",
    " - The L2 regularization weight (`\"L2_penalty\"`), which should be a small number and can be increased if numerical issues arise.\n",
    " - The weight of the forces compared to energies in the overall loss. If `\"loss_lerp_weight\"` is closer to 1, the forces have more weight -- which is typically the desired configuration -- while when it is set closer to 0, the energies have more weight.\n",
    "\n",
    " It is generally very easy and fast to do a hyperparameter search on these parameters as we will see later in this notebook.\n",
    " For the sake of simplicity here they are defined explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_params = {\n",
    "    \"l2_penalty\": [1e-5],\n",
    "    \"loss_lerp_weight\": [0.8],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate classes for the model and for the training algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = FrankenPotential(\n",
    "    gnn_config=gnn_config,\n",
    "    rf_config=rf_config,\n",
    "    num_species=2,        # H and O\n",
    "    jac_chunk_size=12,    # chosen to fit in the T4 GPU on colab. You can set it to \"auto\" to adapt to available GPU memory.\n",
    ")\n",
    "trainer = RandomFeaturesTrainer(\n",
    "    train_dataloader=train_dl_8,\n",
    "    save_every_model=False,\n",
    "    device=device,\n",
    "    save_fmaps=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the `fit` method to **train the model**. A separate model for all parameter possibilities in the `solver_params` dictionary is trained, but note that in this case a single possibility has been specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Patching e3n '_spherical_harmonics' function failed unexpectedly. This may or may not be a problem.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7c5d5cc44742f58cddac5b6bb917a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing dataset statistics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    covs+coeffs      | 0.3 cfgs/s |  88% | 1 x NVIDIA RTX 6000 Ada Generation"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`leading_eig` normalization has high memory usage. If you encounter OOM errors try to disable it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   least-squares     | 910.4 models/s | 1 x NVIDIA RTX 6000 Ada Generationion"
     ]
    }
   ],
   "source": [
    "logs, weights = trainer.fit(model, solver_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model to get **predictions** on the whole validation set. We record simple metrics such as energy and force MAE.\n",
    "\n",
    "> NOTE: The **forces_mode** parameter can be set to either \"torch.func\" or \"torch.autograd\". The two have different performance characteristics: \"torch.func\" is more suitable when there are many different models being trained (i.e. many different solver hyperparameters) as it can effectively run these in a batch way. When only a few models are present, \"torch.autograd\" can be much faster. In our testing the cutoff between the two is approximately at 100 models, but this may vary greatly depending on hardware characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ba6783c03b436b9638be60dc4f7f4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting energies and forces:   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "energy_mae = franken.metrics.init_metric(\"energy_MAE\", device=device)\n",
    "forces_mae = franken.metrics.init_metric(\"forces_MAE\", device=device)\n",
    "for atom_data, targets in tqdm(val_dl, desc=\"Predicting energies and forces\"):\n",
    "    # Move the data to the GPU\n",
    "    atom_data = atom_data.to(device=device)\n",
    "    targets = targets.to(device=device)\n",
    "    pred_e, pred_f = model.energy_and_forces(\n",
    "        atom_data,\n",
    "        weights=weights,\n",
    "        forces_mode=\"torch.autograd\",\n",
    "        add_energy_shift=True\n",
    "    )\n",
    "    predictions = Target(pred_e, pred_f)\n",
    "    energy_mae.update(predictions, targets)\n",
    "    forces_mae.update(predictions, targets)\n",
    "energy_mae = energy_mae.compute()\n",
    "forces_mae = forces_mae.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energy error = tensor([1.3339, 1.6966, 9.4363, 0.6200, 0.6260, 1.1368, 0.4391, 0.4357, 0.4207,\n",
      "        0.3163, 0.3162, 0.3226, 0.3731, 0.3927, 0.4086, 0.3977, 0.4172, 0.4297,\n",
      "        0.4022, 0.4164, 0.4399], device='cuda:0') eV/atom\n",
      "Forces error = tensor([35.2299, 34.8653, 34.5746, 29.6398, 29.4364, 29.2765, 26.7001, 26.6049,\n",
      "        26.5321, 25.6087, 25.5922, 25.5748, 25.5772, 25.5806, 25.5855, 25.6596,\n",
      "        25.6493, 25.6663, 25.7061, 25.6690, 25.6730], device='cuda:0') eV/atom\n"
     ]
    }
   ],
   "source": [
    "print(f\"Energy error = {energy_mae} eV/atom\")\n",
    "print(f\"Forces error = {forces_mae} eV/atom\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "\n",
    "Now we will use the automatic hyperparameter tuning interface to franken which can find\n",
    "the best solver hyperparameters, as well as the best kernel parameters. The `autotune` interface\n",
    "can be used from scripts like here or from the command line (through the `franken.autotune` command), \n",
    "and also takes care of saving the trained model to disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalent of running:\n",
    "```bash\n",
    "franken.autotune preset=gaussian-MACE-L0 \\\n",
    "    dataset.dataset_name='water' \\\n",
    "    dataset.train_subsample.num=8 \\\n",
    "    dataset.train_subsample.rng_seed=42 \\\n",
    "    hyperparameters.random_features.length_scale.start=1 \\\n",
    "    hyperparameters.random_features.length_scale.stop=30 \\\n",
    "    hyperparameters.random_features.length_scale.num=10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 10:33:40.078 DEBUG (rank 0): console_logging_level: DEBUG\n",
      "distributed: True\n",
      "seed: 1337\n",
      "dataset:\n",
      "    dataset_name: water\n",
      "    test_path: null\n",
      "    train_path: null\n",
      "    train_subsample:\n",
      "      num: 8\n",
      "      rng_seed: 42\n",
      "    val_path: null\n",
      "franken:\n",
      "    atomic_energies: null\n",
      "    gnn_backbone_id: MACE-L0\n",
      "    interaction_block: 2\n",
      "    jac_chunk_size: auto\n",
      "    kernel_type: multiscale-gaussian\n",
      "    scale_by_Z: true\n",
      "hyperparameters:\n",
      "    random_features:\n",
      "      length_scale:\n",
      "        _target_: numpy.linspace\n",
      "        num: 10\n",
      "        start: 1\n",
      "        stop: 10\n",
      "      length_scale_high: 30\n",
      "      length_scale_low: 1\n",
      "      length_scale_num: 5\n",
      "      num_random_features: 1024\n",
      "    solver:\n",
      "      L2_penalty:\n",
      "        _target_: numpy.logspace\n",
      "        num: 6\n",
      "        start: -11\n",
      "        stop: -6\n",
      "      loss_lerp_weight:\n",
      "        _target_: numpy.linspace\n",
      "        num: 3\n",
      "        start: 0.01\n",
      "        stop: 0.99\n",
      "paths:\n",
      "    root_dir: .\n",
      "    run_dir: ./experiments/water/sample_complexity/MACE-L0-2/1024_rfs/8_subsamples/multiscale-gaussian\n",
      "trainer:\n",
      "    _target_: franken.trainers.RandomFeaturesTrainer\n",
      "    dtype: float64\n",
      "    random_features_normalization: leading_eig\n",
      "    save_every_model: false\n",
      "    save_fmaps: false\n",
      "\n",
      "2025-03-27 10:33:40.078 INFO (rank 0): Initializing default cache directory at /home/gmeanti/.franken\n",
      "2025-03-27 10:33:40.088 INFO (rank 0): Model already exists locally at /home/gmeanti/.franken/gnn_checkpoints/mace/2023-12-10-mace-128-L0_energy_epoch-249.model. No download needed.\n",
      "2025-03-27 10:33:40.089 INFO (rank 0): Run folder: experiments/water/sample_complexity/MACE-L0-2/1024_rfs/8_subsamples/multiscale-gaussian/run_250327_103340_63b4faa0\n",
      "cuequivariance or cuequivariance_torch is not available. Cuequivariance acceleration will be disabled.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fab71520cfe49fb890baabc24b5ee94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ASE -> MACE (train):   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ea92dae9624c18b2dca87bcf5c6eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ASE -> MACE (val):   0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-27 10:33:43.375 DEBUG (rank 0): Autotune iteration with RF parameters {'length_scale': 1.0, 'length_scale_high': 30, 'length_scale_low': 1, 'length_scale_num': 5, 'num_random_features': 1024}\n",
      "2025-03-27 10:33:43.445 WARNING (rank 0): Tried to initialize MultiScaleOrthogonalRFF with invalid parameters: {'length_scale': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c227b690d47a4d15a41a7cd9addce28f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing dataset statistics:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_cfg = DatasetConfig(name=\"water\", max_train_samples=8)\n",
    "solver_cfg = SolverConfig(\n",
    "    l2_penalty=HPSearchConfig(start=-11, stop=-5, num=10, scale='log'),  # equivalent of numpy.logspace\n",
    "    force_weight=HPSearchConfig(start=0.01, stop=0.99, num=10, scale='linear'),  # equivalent of numpy.linspace\n",
    ")\n",
    "autotune_cfg = AutotuneConfig(\n",
    "    dataset=dataset_cfg,\n",
    "    solver=solver_cfg,\n",
    "    backbone=gnn_config,\n",
    "    rfs=rf_config,\n",
    "    seed=42,\n",
    "    run_dir=\".\"\n",
    ")\n",
    "\n",
    "autotune(autotune_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running autotune the model has been saved to disk, along with useful statistics about the autotune process.\n",
    "We will plot these statistics below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Molecular dynamics\n",
    "\n",
    "The saved potentials model can now be used for a simple molecular dynamics experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ase\n",
    "import ase.md\n",
    "import ase.io\n",
    "import ase.units\n",
    "\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from franken.mdgen.mdgen_utils import FrankenMDLogger\n",
    "from franken.utils.misc import setup_logger\n",
    "from franken.calculators.ase_calc import FrankenCalculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_logger(logging.INFO)\n",
    "md_length_ns = 0.5\n",
    "timestep_fs = 0.5\n",
    "temperature_K = 325\n",
    "friction = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = DATASET_REGISTRY.get_path(\"water\", \"train\", CacheDir.get())\n",
    "initial_configuration = ase.io.read(data_path, index=567)\n",
    "# Trajectory will contain the output data\n",
    "output_path = Path(\".\")\n",
    "trajectory_path = output_path / f\"md_output.traj\"\n",
    "trajectory = ase.io.Trajectory(trajectory_path, \"w\", initial_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\".\")\n",
    "calc = FrankenCalculator(model_path, device=device)\n",
    "integrator = ase.md.Langevin(\n",
    "    temperature_K=temperature_K,\n",
    "    friction=friction / ase.units.fs,\n",
    "    atoms=initial_configuration,\n",
    "    timestep=timestep_fs * ase.units.fs\n",
    ")\n",
    "md_logger = FrankenMDLogger(\n",
    "    dynamics=integrator,\n",
    "    atoms=initial_configuration,\n",
    "    stress=False,\n",
    "    peratom=False,\n",
    ")\n",
    "# nan checker needs to go first to be effective.\n",
    "integrator.attach(md_logger, interval=100)\n",
    "integrator.attach(trajectory.write, interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrator.run(md_length_ns * 1e6 / timestep_fs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
